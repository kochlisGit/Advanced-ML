{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Active-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfkeKg5ZTzbeSxvzB/+6PA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kochlisGit/Advanced-ML/blob/main/Active_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Title**\n",
        "\n",
        "Assignment 2 - Multi-Label Learning\n",
        "\n",
        "**Course**\n",
        "\n",
        "Advanced Machine Learning Topics - Master in Artificial Intelligence\n",
        "\n",
        "**Authors**:\n",
        "\n",
        "1.   Anastasia Papadopoulou\n",
        "2.   Vasileios Kochliaridis"
      ],
      "metadata": {
        "id": "8cU8hoaMxaVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, we are going to explore some active learning techniques. Since it costs both money and time to label large volumes of data, active learning is a valuable option in cases where there are many unlabeled data and we may only want to intelligently label the most informative instances.\n",
        "\n",
        "We are going to separate our dataset to an unlabeled set and a test set. Then, the active learner algorithm will pick the most useful samples, using an uncertainty sampling strategies. For this purpose, we are going to use the Library **modAL**, to apply the following techniques:\n",
        "\n",
        "1.   **Uncertainty Sampling**\n",
        "2.   **Mergin Sampling**\n",
        "3.   **Entropy Sampling**\n",
        "\n",
        "Then we will compare the above strategies with a **random sampling strategy**."
      ],
      "metadata": {
        "id": "iPxX-NTaxKOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPWscmRew1Yo",
        "outputId": "f8f140eb-839e-4471-82a2-5d2cba899da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modAL\n",
            "  Downloading modAL-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (3.1.0)\n",
            "Installing collected packages: modAL\n",
            "Successfully installed modAL-0.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install modAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "diabetes_data = datasets.load_breast_cancer(as_frame=True)\n",
        "inputs = diabetes_data['data']\n",
        "targets = diabetes_data['target']\n",
        "\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "YLGbfriz0u-h",
        "outputId": "18b13fc9-12a6-46df-c776-e8a8fde9a7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "0                   0.07871  ...        25.380          17.33   \n",
              "1                   0.05667  ...        24.990          23.41   \n",
              "2                   0.05999  ...        23.570          25.53   \n",
              "3                   0.09744  ...        14.910          26.50   \n",
              "4                   0.05883  ...        22.540          16.67   \n",
              "..                      ...  ...           ...            ...   \n",
              "564                 0.05623  ...        25.450          26.40   \n",
              "565                 0.05533  ...        23.690          38.25   \n",
              "566                 0.05648  ...        18.980          34.12   \n",
              "567                 0.07016  ...        25.740          39.42   \n",
              "568                 0.05884  ...         9.456          30.37   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "0             184.60      2019.0           0.16220            0.66560   \n",
              "1             158.80      1956.0           0.12380            0.18660   \n",
              "2             152.50      1709.0           0.14440            0.42450   \n",
              "3              98.87       567.7           0.20980            0.86630   \n",
              "4             152.20      1575.0           0.13740            0.20500   \n",
              "..               ...         ...               ...                ...   \n",
              "564           166.10      2027.0           0.14100            0.21130   \n",
              "565           155.00      1731.0           0.11660            0.19220   \n",
              "566           126.70      1124.0           0.11390            0.30940   \n",
              "567           184.60      1821.0           0.16500            0.86810   \n",
              "568            59.16       268.6           0.08996            0.06444   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "0             0.7119                0.2654          0.4601   \n",
              "1             0.2416                0.1860          0.2750   \n",
              "2             0.4504                0.2430          0.3613   \n",
              "3             0.6869                0.2575          0.6638   \n",
              "4             0.4000                0.1625          0.2364   \n",
              "..               ...                   ...             ...   \n",
              "564           0.4107                0.2216          0.2060   \n",
              "565           0.3215                0.1628          0.2572   \n",
              "566           0.3403                0.1418          0.2218   \n",
              "567           0.9387                0.2650          0.4087   \n",
              "568           0.0000                0.0000          0.2871   \n",
              "\n",
              "     worst fractal dimension  \n",
              "0                    0.11890  \n",
              "1                    0.08902  \n",
              "2                    0.08758  \n",
              "3                    0.17300  \n",
              "4                    0.07678  \n",
              "..                       ...  \n",
              "564                  0.07115  \n",
              "565                  0.06637  \n",
              "566                  0.07820  \n",
              "567                  0.12400  \n",
              "568                  0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6469047c-0119-4ee5-9ddb-dc27e92364e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6469047c-0119-4ee5-9ddb-dc27e92364e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6469047c-0119-4ee5-9ddb-dc27e92364e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6469047c-0119-4ee5-9ddb-dc27e92364e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_3ZgkG2258P",
        "outputId": "2c2c1d40-08c2-45dc-ea90-0eebaa7c66a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "564    0\n",
              "565    0\n",
              "566    0\n",
              "567    0\n",
              "568    1\n",
              "Name: target, Length: 569, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "inputs = inputs.to_numpy()\n",
        "targets = targets.to_numpy()\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.5, train_size=0.5, random_state=RANDOM_STATE, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MVk6q6M3ETs",
        "outputId": "84b20665-8c90-410e-b749-9f5651a887b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((284, 30), (284,), (285, 30), (285,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from modAL.models import ActiveLearner\n",
        "from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling\n",
        "\n",
        "N_QUERIES = 50\n",
        "\n",
        "query_strategies = {\n",
        "    'Uncertainty Sampling': uncertainty_sampling,\n",
        "    'Margin Sampling': margin_sampling,\n",
        "    'Entropy Sampling': entropy_sampling\n",
        "}\n",
        "\n",
        "for query_strategy_name, query_strategy in query_strategies.items():\n",
        "  print('\\n--- Using {} strategy ---'.format(query_strategy_name))\n",
        "\n",
        "  classifiers = {\n",
        "      'Random Forest Classifier': RandomForestClassifier(random_state=RANDOM_STATE),\n",
        "      'Support Vector Classifier': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "      'Naive Bayes Classifier': GaussianNB()\n",
        "  }\n",
        "\n",
        "  for classifier_name, classifier in classifiers.items():\n",
        "    learner = ActiveLearner(\n",
        "        estimator=classifier,\n",
        "        query_strategy=query_strategy,\n",
        "        X_training=x_train, y_training=y_train\n",
        "    )\n",
        "\n",
        "    selected_samples = []\n",
        "    \n",
        "    for i in range(N_QUERIES):\n",
        "      query_idx, query_instance = learner.query(x_train)\n",
        "      selected_samples.append(query_idx[0])\n",
        "      learner.teach(query_instance, y_train[query_idx])\n",
        "\n",
        "    y_pred = learner.predict(x_test)\n",
        "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "\n",
        "    print('\\nEvaluating {}'.format(classifier_name))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    selected_samples.sort()\n",
        "    print('\\nSamples selected:', selected_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwZ1Kqy737vA",
        "outputId": "631b0f5c-b4a2-407e-e0b6-ae137f3f5165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Using Uncertainty Sampling strategy ---\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       101\n",
            "           1       0.96      0.98      0.97       184\n",
            "\n",
            "    accuracy                           0.96       285\n",
            "   macro avg       0.96      0.95      0.96       285\n",
            "weighted avg       0.96      0.96      0.96       285\n",
            "\n",
            "\n",
            "Samples selected: [15, 15, 24, 30, 37, 39, 42, 43, 46, 46, 49, 52, 52, 53, 62, 66, 77, 95, 95, 96, 96, 102, 117, 118, 131, 141, 141, 150, 150, 152, 154, 164, 165, 167, 168, 170, 170, 211, 222, 224, 226, 235, 239, 250, 251, 251, 256, 262, 265, 276]\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       101\n",
            "           1       0.89      0.99      0.94       184\n",
            "\n",
            "    accuracy                           0.92       285\n",
            "   macro avg       0.94      0.88      0.90       285\n",
            "weighted avg       0.92      0.92      0.91       285\n",
            "\n",
            "\n",
            "Samples selected: [8, 8, 8, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131]\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92       101\n",
            "           1       0.95      0.97      0.96       184\n",
            "\n",
            "    accuracy                           0.94       285\n",
            "   macro avg       0.94      0.93      0.94       285\n",
            "weighted avg       0.94      0.94      0.94       285\n",
            "\n",
            "\n",
            "Samples selected: [6, 6, 62, 62, 62, 62, 62, 62, 62, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 152, 152, 168, 168, 168, 251, 251, 251, 251, 251, 251, 251]\n",
            "\n",
            "--- Using Margin Sampling strategy ---\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       101\n",
            "           1       0.96      0.98      0.97       184\n",
            "\n",
            "    accuracy                           0.96       285\n",
            "   macro avg       0.96      0.95      0.96       285\n",
            "weighted avg       0.96      0.96      0.96       285\n",
            "\n",
            "\n",
            "Samples selected: [15, 15, 24, 30, 37, 39, 42, 43, 46, 46, 49, 52, 52, 53, 62, 66, 77, 95, 95, 96, 96, 102, 117, 118, 131, 141, 141, 150, 150, 152, 154, 164, 165, 167, 168, 170, 170, 211, 222, 224, 226, 235, 239, 250, 251, 251, 256, 262, 265, 276]\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       101\n",
            "           1       0.89      0.99      0.94       184\n",
            "\n",
            "    accuracy                           0.92       285\n",
            "   macro avg       0.94      0.88      0.90       285\n",
            "weighted avg       0.92      0.92      0.91       285\n",
            "\n",
            "\n",
            "Samples selected: [8, 8, 8, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131]\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92       101\n",
            "           1       0.95      0.97      0.96       184\n",
            "\n",
            "    accuracy                           0.94       285\n",
            "   macro avg       0.94      0.93      0.94       285\n",
            "weighted avg       0.94      0.94      0.94       285\n",
            "\n",
            "\n",
            "Samples selected: [6, 6, 62, 62, 62, 62, 62, 62, 62, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 152, 152, 168, 168, 168, 251, 251, 251, 251, 251, 251, 251]\n",
            "\n",
            "--- Using Entropy Sampling strategy ---\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       101\n",
            "           1       0.96      0.98      0.97       184\n",
            "\n",
            "    accuracy                           0.96       285\n",
            "   macro avg       0.96      0.95      0.96       285\n",
            "weighted avg       0.96      0.96      0.96       285\n",
            "\n",
            "\n",
            "Samples selected: [15, 15, 24, 30, 37, 39, 42, 43, 46, 46, 49, 52, 52, 53, 62, 66, 77, 95, 95, 96, 96, 102, 117, 118, 131, 141, 141, 150, 150, 152, 154, 164, 165, 167, 168, 170, 170, 211, 222, 224, 226, 235, 239, 250, 251, 251, 256, 262, 265, 276]\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       101\n",
            "           1       0.89      0.99      0.94       184\n",
            "\n",
            "    accuracy                           0.92       285\n",
            "   macro avg       0.94      0.88      0.90       285\n",
            "weighted avg       0.92      0.92      0.91       285\n",
            "\n",
            "\n",
            "Samples selected: [8, 8, 8, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131]\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92       101\n",
            "           1       0.95      0.97      0.96       184\n",
            "\n",
            "    accuracy                           0.94       285\n",
            "   macro avg       0.94      0.93      0.94       285\n",
            "weighted avg       0.94      0.94      0.94       285\n",
            "\n",
            "\n",
            "Samples selected: [6, 6, 62, 62, 62, 62, 62, 62, 62, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 152, 152, 168, 168, 168, 251, 251, 251, 251, 251, 251, 251]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try random sampling and no sampling at all."
      ],
      "metadata": {
        "id": "a66Ap3gTROFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(RANDOM_STATE)\n",
        "random_indices = random.sample(range(x_train.shape[0]), k=N_QUERIES)\n",
        "\n",
        "x_random = x_train[random_indices]\n",
        "y_random = y_train[random_indices]\n",
        "\n",
        "classifiers = {\n",
        "    'Random Forest Classifier': RandomForestClassifier(random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': GaussianNB()\n",
        "}\n",
        "\n",
        "for classifier_name, classifier in classifiers.items():\n",
        "  classifier.fit(x_random, y_random)\n",
        "  y_pred = classifier.predict(x_test)\n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpJnLKc3PiLd",
        "outputId": "1d3403bb-721a-4685-f8be-51211c196114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87       101\n",
            "           1       0.92      0.93      0.93       184\n",
            "\n",
            "    accuracy                           0.91       285\n",
            "   macro avg       0.90      0.90      0.90       285\n",
            "weighted avg       0.91      0.91      0.91       285\n",
            "\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.73      0.84       101\n",
            "           1       0.87      0.99      0.93       184\n",
            "\n",
            "    accuracy                           0.90       285\n",
            "   macro avg       0.93      0.86      0.88       285\n",
            "weighted avg       0.91      0.90      0.90       285\n",
            "\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.90       101\n",
            "           1       0.94      0.95      0.94       184\n",
            "\n",
            "    accuracy                           0.93       285\n",
            "   macro avg       0.92      0.92      0.92       285\n",
            "weighted avg       0.93      0.93      0.93       285\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    'Random Forest Classifier': RandomForestClassifier(random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': SVC(probability=True, random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': GaussianNB()\n",
        "}\n",
        "\n",
        "for classifier_name, classifier in classifiers.items():\n",
        "  classifier.fit(x_train, y_train)\n",
        "  y_pred = classifier.predict(x_test)\n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70CYtT2eRFyb",
        "outputId": "8f8cfd34-a4c3-481c-8043-f402cce40751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.94       101\n",
            "           1       0.96      0.97      0.96       184\n",
            "\n",
            "    accuracy                           0.95       285\n",
            "   macro avg       0.95      0.95      0.95       285\n",
            "weighted avg       0.95      0.95      0.95       285\n",
            "\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       101\n",
            "           1       0.89      0.99      0.94       184\n",
            "\n",
            "    accuracy                           0.92       285\n",
            "   macro avg       0.94      0.88      0.90       285\n",
            "weighted avg       0.92      0.92      0.91       285\n",
            "\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91       101\n",
            "           1       0.95      0.96      0.95       184\n",
            "\n",
            "    accuracy                           0.94       285\n",
            "   macro avg       0.93      0.93      0.93       285\n",
            "weighted avg       0.94      0.94      0.94       285\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we present the **accuracy** results of *Active Learning vs Random Sampling and No Sampling*.\n",
        "\n",
        "Classifier     | No Sampling | Random | Uncertainty | Margin | Entropy |\n",
        "---------------|-------------|--------|-------------|--------|---------|\n",
        "Random Forest  | 0.95        | 0.91   | 0.96        | 0.96   | 0.96    |\n",
        "Support Vector | 0.92        | 0.90   | 0.92        | 0.92   | 0.92    |\n",
        "Naive Bayes    | 0.94        | 0.93   | 0.94        | 0.94   | 0.94    |\n",
        "\n",
        "It is pretty obvious that random resampling strategy has the worst performance. Also, active learning methods perform as good as training with the entire dataset."
      ],
      "metadata": {
        "id": "IYTwVDiaQ8bL"
      }
    }
  ]
}