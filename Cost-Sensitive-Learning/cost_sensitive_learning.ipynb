{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cost-Sensitive Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYgwiSmHmwJIx75Ha9L0nr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kochlisGit/Advanced-ML/blob/main/Cost-Sensitive-Learning/cost_sensitive_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2QRpfranjap5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries.\n",
        "\n",
        "!pip install scikit-learn==0.22.2.post1\n",
        "!pip install costcla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUFyveC_dQSF",
        "outputId": "b8f2184a-d490-4112-8ab6-6122239e9c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.22.2.post1\n",
            "  Downloading scikit_learn-0.22.2.post1-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.21.5)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.22.2.post1\n",
            "Collecting costcla\n",
            "  Downloading costcla-0.6-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from costcla) (1.21.5)\n",
            "Collecting pyea>=0.2\n",
            "  Downloading pyea-0.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: pandas>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from costcla) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0b2 in /usr/local/lib/python3.7/dist-packages (from costcla) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.14.0->costcla) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15.0b2->costcla) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15.0b2->costcla) (1.1.0)\n",
            "Building wheels for collected packages: pyea\n",
            "  Building wheel for pyea (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyea: filename=pyea-0.2-py3-none-any.whl size=6018 sha256=ceeb9d902081721ece79e8fa223d88a020ce2a77bc24562f39a9abd0ccc1f1b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/c7/f9/c43bd31860d7235d875091659066bf793ea300fd0621156737\n",
            "Successfully built pyea\n",
            "Installing collected packages: pyea, costcla\n",
            "Successfully installed costcla-0.6 pyea-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "x7YKPnRS_nxU",
        "outputId": "42682b1b-6e74-48ce-bdee-68b05464c9b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Age  Sex  Chest Pain Type  Resting Blood Pressure  \\\n",
              "0    70.0  1.0              4.0                   130.0   \n",
              "1    67.0  0.0              3.0                   115.0   \n",
              "2    57.0  1.0              2.0                   124.0   \n",
              "3    64.0  1.0              4.0                   128.0   \n",
              "4    74.0  0.0              2.0                   120.0   \n",
              "..    ...  ...              ...                     ...   \n",
              "265  52.0  1.0              3.0                   172.0   \n",
              "266  44.0  1.0              2.0                   120.0   \n",
              "267  56.0  0.0              2.0                   140.0   \n",
              "268  57.0  1.0              4.0                   140.0   \n",
              "269  67.0  1.0              4.0                   160.0   \n",
              "\n",
              "     Serum Cholesterol (mg/dl)  Fasting Blood Sugar > 120 mg/dl  \\\n",
              "0                        322.0                              0.0   \n",
              "1                        564.0                              0.0   \n",
              "2                        261.0                              0.0   \n",
              "3                        263.0                              0.0   \n",
              "4                        269.0                              0.0   \n",
              "..                         ...                              ...   \n",
              "265                      199.0                              1.0   \n",
              "266                      263.0                              0.0   \n",
              "267                      294.0                              0.0   \n",
              "268                      192.0                              0.0   \n",
              "269                      286.0                              0.0   \n",
              "\n",
              "     Resting Electrocardiographic Results  Maximum Heart Rate Achieved  \\\n",
              "0                                     2.0                        109.0   \n",
              "1                                     2.0                        160.0   \n",
              "2                                     0.0                        141.0   \n",
              "3                                     0.0                        105.0   \n",
              "4                                     2.0                        121.0   \n",
              "..                                    ...                          ...   \n",
              "265                                   0.0                        162.0   \n",
              "266                                   0.0                        173.0   \n",
              "267                                   2.0                        153.0   \n",
              "268                                   0.0                        148.0   \n",
              "269                                   2.0                        108.0   \n",
              "\n",
              "     Exercise Induced Angina  Depression by Exercise/Rest  \\\n",
              "0                        0.0                          2.4   \n",
              "1                        0.0                          1.6   \n",
              "2                        0.0                          0.3   \n",
              "3                        1.0                          0.2   \n",
              "4                        1.0                          0.2   \n",
              "..                       ...                          ...   \n",
              "265                      0.0                          0.5   \n",
              "266                      0.0                          0.0   \n",
              "267                      0.0                          1.3   \n",
              "268                      0.0                          0.4   \n",
              "269                      1.0                          1.5   \n",
              "\n",
              "     Slope of Peak Exercise  Number of Major Vessels  Thal  Presence  \n",
              "0                       2.0                      3.0   3.0         2  \n",
              "1                       2.0                      0.0   7.0         1  \n",
              "2                       1.0                      0.0   7.0         2  \n",
              "3                       2.0                      1.0   7.0         1  \n",
              "4                       1.0                      1.0   3.0         1  \n",
              "..                      ...                      ...   ...       ...  \n",
              "265                     1.0                      0.0   7.0         1  \n",
              "266                     1.0                      0.0   7.0         1  \n",
              "267                     2.0                      0.0   3.0         1  \n",
              "268                     2.0                      0.0   6.0         1  \n",
              "269                     2.0                      3.0   3.0         2  \n",
              "\n",
              "[270 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48501f63-2023-46be-8317-c1a2611b5fcb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Chest Pain Type</th>\n",
              "      <th>Resting Blood Pressure</th>\n",
              "      <th>Serum Cholesterol (mg/dl)</th>\n",
              "      <th>Fasting Blood Sugar &gt; 120 mg/dl</th>\n",
              "      <th>Resting Electrocardiographic Results</th>\n",
              "      <th>Maximum Heart Rate Achieved</th>\n",
              "      <th>Exercise Induced Angina</th>\n",
              "      <th>Depression by Exercise/Rest</th>\n",
              "      <th>Slope of Peak Exercise</th>\n",
              "      <th>Number of Major Vessels</th>\n",
              "      <th>Thal</th>\n",
              "      <th>Presence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>564.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>261.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>269.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>162.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>44.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>56.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>57.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>192.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>67.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>270 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48501f63-2023-46be-8317-c1a2611b5fcb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48501f63-2023-46be-8317-c1a2611b5fcb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48501f63-2023-46be-8317-c1a2611b5fcb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Downloading Statlog (Heart) Data Set from UCI\n",
        "import pandas as pd\n",
        "\n",
        "column_names = [\n",
        "                'Age',\n",
        "                'Sex',\n",
        "                'Chest Pain Type',\n",
        "                'Resting Blood Pressure',\n",
        "                'Serum Cholesterol (mg/dl)',\n",
        "                'Fasting Blood Sugar > 120 mg/dl',\n",
        "                'Resting Electrocardiographic Results',\n",
        "                'Maximum Heart Rate Achieved',\n",
        "                'Exercise Induced Angina',\n",
        "                'Depression by Exercise/Rest',\n",
        "                'Slope of Peak Exercise',\n",
        "                'Number of Major Vessels',\n",
        "                'Thal',\n",
        "                'Presence'\n",
        "]\n",
        "delimiter = ' '\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat', names=column_names, delimiter=delimiter)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating training dataset.\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "TEST_SIZE = 0.3\n",
        "RANDOM_STATE = 0\n",
        "SHUFFLE = True\n",
        "\n",
        "data = data.dropna()\n",
        "targets = data['Presence'].replace({1: 0, 2: 1})\n",
        "inputs = data.drop(columns=['Presence'])\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(inputs, targets, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=SHUFFLE)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_iSzCMsB-LQ",
        "outputId": "65f48a91-6e31-43ee-a166-5f6a29f3d026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((189, 13), (189,), (81, 13), (81,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, we have 189 samples available for training and 81 samples for evaluation. Unfortunately, we are not provided with enough data, so our training will be extremely difficult. The special issue about this dataset is the fact that the classifier that we are going to build is intended to be used for medical purposes, so we have different costs for each kind of prediction. For example, a classifier that outputs lots of **False Positives** could be dangerous and even inappropriate to use."
      ],
      "metadata": {
        "id": "bQ-2BbhM4mwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training classifiers & Evaluating using the Cost\n",
        "\n",
        "from sklearn import ensemble, svm, naive_bayes\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "# Defining the cost according to the recommended cost matrix of the dataset.\n",
        "# It is important not to output \"absence\" if a patient suffers from a heart desease.\n",
        "\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 5\n",
        "FN = 1\n",
        "cost = np.float32([[TN , FP], [FN, TP]])\n",
        "\n",
        "# Defining the classifiers.\n",
        "\n",
        "classifiers = {\n",
        "    'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': svm.LinearSVC(random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "}\n",
        "\n",
        "# Training & Evaluating the classifiers.\n",
        "\n",
        "for classifier_name, clf in classifiers.items():\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "  cost_loss = np.sum(confusion_matrix * cost)\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))\n",
        "  print('Cost loss = {}'.format(cost_loss))\n",
        "  print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[1][1],\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0]\n",
        "  ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3GVNQrPJ6r4",
        "outputId": "9ee4c68f-801e-40a4-be30-d44eac345b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82        48\n",
            "           1       0.72      0.85      0.78        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.81      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 36.0\n",
            "By Confusion Matrix: TP: 28, TN: 37, FP: 5, FN: 11\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87        48\n",
            "           1       0.80      0.85      0.82        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.85      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 28, TN: 41, FP: 5, FN: 7\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83        48\n",
            "           1       0.74      0.79      0.76        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.80      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 44.0\n",
            "By Confusion Matrix: TP: 26, TN: 39, FP: 7, FN: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, in this dataset the Random Forest classifier has the highest `Accuracy` along with Naive Bayes Classifier and SVC performs the worst. More specifically, Random Forest has `Accuracy = 0.80, F1-Score(0) = 0.82, F1-Score(1) = 0.78` and Naive Bayes Classifier has `Accuracy = 0.80, F1-Score(0) = 0.83, F1-Score(1) = 0.76` percent However, we are mainly insterested in the classifier with the minimum cost loss. In this case, the rankings are:\n",
        "\n",
        "1.   Support Vector Classifier:` Cost loss = 36`\n",
        "2.   Random Forest Classifier: `Cost loss = 91`\n",
        "3.   Naive Bayes Classifier: `Cost loss = 41 `\n",
        "\n",
        "However, a model with cost of 36 seems to be quite high for medical applications. We can minimize the cost loss even further using the following techniques:\n",
        "\n",
        "\n",
        "\n",
        "1.   Calibration\n",
        "2.   Weighting\n",
        "3.   Weighting + Calibration on Data\n",
        "4.   Rebalancing\n"
      ],
      "metadata": {
        "id": "pgJespblTduZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Calibration (Sigmoid/Isotonic)**"
      ],
      "metadata": {
        "id": "6pfTq26dY1HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Adding calibration to the model's output.\n",
        "# Calibration methods: a) Sigmoid, b) Isotonic\n",
        "\n",
        "from sklearn import calibration\n",
        "from costcla.models import BayesMinimumRiskClassifier\n",
        "from costcla.metrics import cost_loss\n",
        "\n",
        "TP = np.zeros((y_test.shape[0], 1))\n",
        "TN = np.zeros((y_test.shape[0], 1))\n",
        "FP = np.full((y_test.shape[0],1), 5)\n",
        "FN = np.full((y_test.shape[0],1), 1)\n",
        "cost_matrix = np.hstack((FP, FN, TP, TN))\n",
        "CV = 3\n",
        "\n",
        "calibration_methods = {\n",
        "    'Sigmoid': 'sigmoid',\n",
        "    'Isotonic': 'isotonic'\n",
        "}\n",
        "\n",
        "\n",
        "for calibration_method, cc in calibration_methods.items():\n",
        "  classifiers = {\n",
        "      'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "      'Support Vector Classifier': svm.SVC(kernel='linear',probability=True, random_state=RANDOM_STATE),\n",
        "      'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "  }\n",
        "\n",
        "  for classifier_name, clf in classifiers.items():\n",
        "    cc_clf = calibration.CalibratedClassifierCV(clf, method=cc, cv=CV)\n",
        "    model = clf.fit(x_train, y_train)\n",
        "    prob_test = model.predict_proba(x_test)\n",
        "    bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "    y_pred = bmr.predict(prob_test, cost_matrix)\n",
        "\n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print('\\nEvaluating {} using {} Calibration'.format(classifier_name, calibration_method))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "    print('Cost loss = {}'.format(loss))\n",
        "\n",
        "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "    print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[1][1],\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0]\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK4mnT1j7VQK",
        "outputId": "8f847a95-3f48-46ac-e4f7-27d4ff7b2fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier using Sigmoid Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.98      0.82        48\n",
            "           1       0.93      0.42      0.58        33\n",
            "\n",
            "    accuracy                           0.75        81\n",
            "   macro avg       0.82      0.70      0.70        81\n",
            "weighted avg       0.80      0.75      0.73        81\n",
            "\n",
            "Cost loss = 24.0\n",
            "By Confusion Matrix: TP: 14, TN: 47, FP: 19, FN: 1\n",
            "\n",
            "Evaluating Support Vector Classifier using Sigmoid Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      1.00      0.83        48\n",
            "           1       1.00      0.42      0.60        33\n",
            "\n",
            "    accuracy                           0.77        81\n",
            "   macro avg       0.86      0.71      0.72        81\n",
            "weighted avg       0.83      0.77      0.74        81\n",
            "\n",
            "Cost loss = 19.0\n",
            "By Confusion Matrix: TP: 14, TN: 48, FP: 19, FN: 0\n",
            "\n",
            "Evaluating Naive Bayes Classifier using Sigmoid Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88        48\n",
            "           1       0.84      0.79      0.81        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.84      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 26, TN: 43, FP: 7, FN: 5\n",
            "\n",
            "Evaluating Random Forest Classifier using Isotonic Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.98      0.82        48\n",
            "           1       0.93      0.42      0.58        33\n",
            "\n",
            "    accuracy                           0.75        81\n",
            "   macro avg       0.82      0.70      0.70        81\n",
            "weighted avg       0.80      0.75      0.73        81\n",
            "\n",
            "Cost loss = 24.0\n",
            "By Confusion Matrix: TP: 14, TN: 47, FP: 19, FN: 1\n",
            "\n",
            "Evaluating Support Vector Classifier using Isotonic Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      1.00      0.83        48\n",
            "           1       1.00      0.42      0.60        33\n",
            "\n",
            "    accuracy                           0.77        81\n",
            "   macro avg       0.86      0.71      0.72        81\n",
            "weighted avg       0.83      0.77      0.74        81\n",
            "\n",
            "Cost loss = 19.0\n",
            "By Confusion Matrix: TP: 14, TN: 48, FP: 19, FN: 0\n",
            "\n",
            "Evaluating Naive Bayes Classifier using Isotonic Calibration\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88        48\n",
            "           1       0.84      0.79      0.81        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.84      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 26, TN: 43, FP: 7, FN: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, using the calibration methods we built a model with a cost of 19, while our best previous model made predictions with a cost of 36. Using the sigmoid or the isotonic calibration method we managed to reduce the cost almost by half. Additionally, all our classifiers performed better in terms of cost minimization. More specifically:\n",
        "\n",
        "Classifier     | No Calibration | Sigmoid Calibration | Isotonic Calibration \n",
        "---------------|----------------|---------------------|---------------------\n",
        "Random Forest  |36              |24                   |24\n",
        "Support Vector |91              |19                   |19\n",
        "Naive Bayes    |44              |32                   |32\n",
        "\n",
        "We also notice that the cost of SVM is dramatically dropped from 91 to 19. This is because SVM overestimates the low probabilities and underestimates the high probabilities, as we have seen in this lecture. By calibrating the classifier, we correctly estimate the probabilities of SVM.\n",
        "\n",
        "Another thing we notice is that by minimizing the risk, we changed the accuracies of the classifiers. The accuracy of SVM and Random Forest has dropped by a very small percentage, however, Naive Bayes reduced its cost while also improved its prediction score metrics."
      ],
      "metadata": {
        "id": "ulKswdT6kg8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Example Weighting**"
      ],
      "metadata": {
        "id": "M5O5ncesY7Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Adding class weights during training.\n",
        "# According to the cost matrix, it is much important that a patient with heart disease is not misclassified.\n",
        "\n",
        "weights = np.full(y_train.shape[0], 1)\n",
        "weights[np.where(y_train == 0)] = 5;\n",
        "\n",
        "classifiers = {\n",
        "    'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': svm.LinearSVC(random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "}\n",
        "\n",
        "# Training & Evaluating the classifiers.\n",
        "\n",
        "for classifier_name, clf in classifiers.items():\n",
        "  clf.fit(x_train, y_train, weights)\n",
        "  y_pred = clf.predict(x_test)\n",
        "\n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "  cost_loss = np.sum(confusion_matrix * cost)\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))\n",
        "  print('Cost loss = {}'.format(cost_loss))\n",
        "\n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "  print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[1][1],\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0]\n",
        "  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYrXfs8Mu0aL",
        "outputId": "f9bc2e8b-8dbb-4469-d0bb-128e7f8f33e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.83        48\n",
            "           1       0.73      0.82      0.77        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.80      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 40.0\n",
            "By Confusion Matrix: TP: 27, TN: 38, FP: 6, FN: 10\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86        48\n",
            "           1       0.83      0.73      0.77        33\n",
            "\n",
            "    accuracy                           0.83        81\n",
            "   macro avg       0.83      0.81      0.82        81\n",
            "weighted avg       0.83      0.83      0.83        81\n",
            "\n",
            "Cost loss = 50.0\n",
            "By Confusion Matrix: TP: 24, TN: 43, FP: 9, FN: 5\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88        48\n",
            "           1       0.84      0.79      0.81        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.84      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 40.0\n",
            "By Confusion Matrix: TP: 26, TN: 43, FP: 7, FN: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, by adding class weights to our training, it improved the overall accuracy metrics of the classifiers. We used the cost matrix to define the weights for each class (**5 for absence and 1 for presence**). However, the costs for each classifier still remains high, so probably needs to be combined with a calibration method."
      ],
      "metadata": {
        "id": "U8HMF3M1B9mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Weighting + Calibration on Data**"
      ],
      "metadata": {
        "id": "aGBAxhZaY9_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus Method: Calibrating the classifiers on the training set with a neural network.\n",
        "# 1. Classifiers will trained, without weights.\n",
        "# 2. Then, the output probabilities will be computed for each training example and will be concatenated with the training inputs.\n",
        "# 3. The new training inputs will be fed into a neural network.\n",
        "# 4. Loss weights will be added to the final classifier.\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def build_model(input_shape):\n",
        "  inputs = tf.keras.layers.Input(input_shape)\n",
        "  x = tf.keras.layers.Dense(units=64, use_bias=False)(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.activations.gelu(x)\n",
        "  outputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
        "  nnet = tf.keras.Model(inputs, outputs)\n",
        "  nnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'], loss_weights=weights)\n",
        "  return nnet\n",
        "\n",
        "\n",
        "classifiers = {\n",
        "    'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': svm.SVC(kernel='linear', probability=True, random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "}\n",
        "\n",
        "for classifier_name, clf in classifiers.items():\n",
        "  # Buildin & Training the model.\n",
        "\n",
        "  model = clf.fit(x_train, y_train)\n",
        "  prob_train = model.predict_proba(x_train)\n",
        "  nnet_train_inputs = np.hstack((x_train, prob_train))\n",
        "  nnet = build_model(input_shape=[nnet_train_inputs.shape[1]])\n",
        "  nnet.fit(nnet_train_inputs, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
        "\n",
        "  # Evaluating the model.\n",
        "\n",
        "  prob_test = model.predict_proba(x_test)\n",
        "  nnet_test_inputs = np.hstack((x_test, prob_test))\n",
        "\n",
        "  y_pred_probabilities = nnet.predict(nnet_test_inputs)\n",
        "  y_pred = np.round(y_pred_probabilities)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "  cost_loss = np.sum(confusion_matrix * cost)\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))\n",
        "  print('Cost loss = {}'.format(cost_loss))\n",
        "  print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[1][1],\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0]\n",
        "  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_MQi-F6C9nw",
        "outputId": "56cea7e8-2c63-4984-81d4-1c62787427e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80        48\n",
            "           1       0.69      0.82      0.75        33\n",
            "\n",
            "    accuracy                           0.78        81\n",
            "   macro avg       0.77      0.78      0.78        81\n",
            "weighted avg       0.79      0.78      0.78        81\n",
            "\n",
            "Cost loss = 42.0\n",
            "By Confusion Matrix: TP: 27, TN: 36, FP: 6, FN: 12\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.58      0.71        48\n",
            "           1       0.60      0.91      0.72        33\n",
            "\n",
            "    accuracy                           0.72        81\n",
            "   macro avg       0.75      0.75      0.72        81\n",
            "weighted avg       0.78      0.72      0.71        81\n",
            "\n",
            "Cost loss = 35.0\n",
            "By Confusion Matrix: TP: 30, TN: 28, FP: 3, FN: 20\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.52      0.67        48\n",
            "           1       0.57      0.94      0.71        33\n",
            "\n",
            "    accuracy                           0.69        81\n",
            "   macro avg       0.75      0.73      0.69        81\n",
            "weighted avg       0.78      0.69      0.69        81\n",
            "\n",
            "Cost loss = 33.0\n",
            "By Confusion Matrix: TP: 31, TN: 25, FP: 2, FN: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we present the final costs by combining example weighting with calibration on dataset:\n",
        "\n",
        "Classifier     | No Cost Min.   | Weighting           | Calibration + Weighting \n",
        "---------------|----------------|---------------------|---------------------\n",
        "Random Forest  |36              |40                   |42\n",
        "Support Vector |91              |50                   |35\n",
        "Naive Bayes    |44              |40                   |33"
      ],
      "metadata": {
        "id": "nmYnouX9W27d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Train/Test Stratification (aka Rebalancing)**"
      ],
      "metadata": {
        "id": "gMQ2dhFmZHrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn import under_sampling, over_sampling\n",
        "\n",
        "sampling_methods = {\n",
        "    'Under Sampling': under_sampling.RandomUnderSampler(sampling_strategy={0: 10, 1: 50}, random_state=RANDOM_STATE),\n",
        "    'Over Sampling': over_sampling.RandomOverSampler(sampling_strategy={0: 1000, 1: 5000}, random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "for method, sampler in sampling_methods.items():\n",
        "  x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "\n",
        "  print('\\n\\n\\n------------- Using {} -------------'.format(method))\n",
        "  print('Training data: {}'.format(Counter(y_rs)))\n",
        "\n",
        "  classifiers = {\n",
        "    'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "    'Support Vector Classifier': svm.LinearSVC(random_state=RANDOM_STATE),\n",
        "    'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "  }\n",
        "\n",
        "  for classifier_name, clf in classifiers.items():\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    \n",
        "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "    cost_loss = np.sum(confusion_matrix * cost)\n",
        "\n",
        "    print('\\nEvaluating {}'.format(classifier_name))\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "    print('Cost loss = {}'.format(cost_loss))\n",
        "    print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[1][1],\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0]\n",
        "    ))\n",
        "\n",
        "print('\\n\\n\\n ------------- Combining methods -------------')\n",
        "\n",
        "sampler = under_sampling.RandomUnderSampler(sampling_strategy={0: 10, 1: 50}, random_state=RANDOM_STATE)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "sampler = over_sampling.RandomOverSampler(sampling_strategy={0: 1000, 1: 5000}, random_state=RANDOM_STATE)\n",
        "x_rs, y_rs = sampler.fit_resample(x_train, y_train)\n",
        "print('Training data: {}'.format(Counter(y_rs)))\n",
        "\n",
        "classifiers = {\n",
        "  'Random Forest Classifier': ensemble.RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
        "  'Support Vector Classifier': svm.LinearSVC(random_state=RANDOM_STATE),\n",
        "  'Naive Bayes Classifier': naive_bayes.GaussianNB()\n",
        "}\n",
        "\n",
        "for classifier_name, clf in classifiers.items():\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred).T\n",
        "  cost_loss = np.sum(confusion_matrix * cost)\n",
        "\n",
        "  print('\\nEvaluating {}'.format(classifier_name))\n",
        "  print(metrics.classification_report(y_test, y_pred))\n",
        "  print('Cost loss = {}'.format(cost_loss))\n",
        "  print('By Confusion Matrix: TP: {}, TN: {}, FP: {}, FN: {}'.format(\n",
        "      confusion_matrix[0][0],\n",
        "      confusion_matrix[0][1],\n",
        "      confusion_matrix[1][0],\n",
        "      confusion_matrix[1][1]\n",
        "  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgIPfBzRZhZq",
        "outputId": "4e123cb3-7bff-4bfd-cb50-87dc0c64c245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "------------- Using Under Sampling -------------\n",
            "Training data: Counter({1: 50, 0: 10})\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82        48\n",
            "           1       0.72      0.85      0.78        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.81      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 36.0\n",
            "By Confusion Matrix: TP: 28, TN: 37, FP: 5, FN: 11\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87        48\n",
            "           1       0.80      0.85      0.82        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.85      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 28, TN: 41, FP: 5, FN: 7\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83        48\n",
            "           1       0.74      0.79      0.76        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.80      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 44.0\n",
            "By Confusion Matrix: TP: 26, TN: 39, FP: 7, FN: 9\n",
            "\n",
            "\n",
            "\n",
            "------------- Using Over Sampling -------------\n",
            "Training data: Counter({1: 5000, 0: 1000})\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82        48\n",
            "           1       0.72      0.85      0.78        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.81      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 36.0\n",
            "By Confusion Matrix: TP: 28, TN: 37, FP: 5, FN: 11\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87        48\n",
            "           1       0.80      0.85      0.82        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.85      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 28, TN: 41, FP: 5, FN: 7\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83        48\n",
            "           1       0.74      0.79      0.76        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.80      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 44.0\n",
            "By Confusion Matrix: TP: 26, TN: 39, FP: 7, FN: 9\n",
            "\n",
            "\n",
            "\n",
            " ------------- Combining methods -------------\n",
            "Training data: Counter({1: 5000, 0: 1000})\n",
            "\n",
            "Evaluating Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82        48\n",
            "           1       0.72      0.85      0.78        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.81      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 36.0\n",
            "By Confusion Matrix: TP: 37, TN: 5, FP: 11, FN: 28\n",
            "\n",
            "Evaluating Support Vector Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87        48\n",
            "           1       0.80      0.85      0.82        33\n",
            "\n",
            "    accuracy                           0.85        81\n",
            "   macro avg       0.85      0.85      0.85        81\n",
            "weighted avg       0.85      0.85      0.85        81\n",
            "\n",
            "Cost loss = 32.0\n",
            "By Confusion Matrix: TP: 41, TN: 5, FP: 7, FN: 28\n",
            "\n",
            "Evaluating Naive Bayes Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83        48\n",
            "           1       0.74      0.79      0.76        33\n",
            "\n",
            "    accuracy                           0.80        81\n",
            "   macro avg       0.80      0.80      0.80        81\n",
            "weighted avg       0.81      0.80      0.80        81\n",
            "\n",
            "Cost loss = 44.0\n",
            "By Confusion Matrix: TP: 39, TN: 7, FP: 9, FN: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we present the final costs of each sampling method separately as well as the combination of both UnderSampling & OverSampling. It also worths mentioning that the accuracy metrics of some classifiers have also been improved. \n",
        "\n",
        "Classifier     | UnderSampling  | OverSampling | UnderSampling + OverSampling \n",
        "---------------|----------------|---------------------|---------------------\n",
        "Random Forest  |36              |36                   |36\n",
        "Support Vector |32              |32                   |32\n",
        "Naive Bayes    |44              |44                   |44"
      ],
      "metadata": {
        "id": "bpZFnxSqfrAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "In this dataset, we have tested 4 cost minimization techniques, which are ***Probability Calibration, Weighting, Calibration on Training Data, Rebalancing**. From all the above methods, *Probability Calibration* achieved remarkable results on minimizing the desired costs. However, all 4 methods successfully managed to drop the prediction costs. In some cases, using these methods after the training of the classifiers increased their score metrics as well, while in some other cases the accuracy dropped by a small percentage. However, in medical applications it is important to build models with the lowest possible False Positive rate, as the decisions of such models could have an impact on human lives."
      ],
      "metadata": {
        "id": "VDXz6vItg6cU"
      }
    }
  ]
}